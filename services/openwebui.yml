# Define the services (containers) that make up your application
services:
  # Define the Open WebUI service
  open-webui:
    # Use the specified Docker image for Open WebUI with Ollama support
    image: ghcr.io/open-webui/open-webui:ollama
    # Assign a specific name to the container
    container_name: open-webui

    # Specify runtime options for GPU access.
    # 'all' grants access to all available GPUs.
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

    # Mount named volumes for persistent data storage
    volumes:
      # Mount the 'ollama' named volume to /root/.ollama inside the container.
      # This is where Ollama stores its models and data.
      - ${DATA_DIR}ollama:/root/.ollama
      # Mount the 'open-webui' named volume to /app/backend/data inside the container.
      # This is where Open WebUI stores its application data.
      - ${DATA_DIR}open-webui:/app/backend/data

    # Set the restart policy to 'always', ensuring the container restarts
    # automatically if it stops or if Docker restarts.
    env_file:
#      - ../.env
      - ../config/owu.env
    environment:
      WEBUI_URL: https://ai.${ROOT_DOMAIN}
      OPENID_PROVIDER_URL: https://auth.${ROOT_DOMAIN}/application/o/ai/.well-known/openid-configuration
      OPENID_REDIRECT_URI: https://ai.${ROOT_DOMAIN}/oauth/oidc/callback
      OAUTH_PROVIDER_NAME: ${SERVER_NAME_FRIENDLY}
    restart: always
    networks:
      - ai_network
      - caddy_network

# Define the named volumes used by the services
volumes:
  # Define the 'ollama' named volume for Ollama data persistence
  ollama:
  # Define the 'open-webui' named volume for Open WebUI data persistence
  open-webui:
